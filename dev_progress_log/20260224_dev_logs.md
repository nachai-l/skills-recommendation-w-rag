# Skills Recommendation — Progress Summary

> Status: **Architecture Defined — Implementation Starting** ✅
> Date: **2026-02-24** (Asia/BKK)
> Repo Structure: Modular (Schema + Batch + Online + Service + API + UI)
> Output Guarantee: Strict Pydantic v2 validation, deterministic hybrid retrieval, JSON-only API contract

---

## 0) What This Framework Is

This project implements a **Skill Recommendation API** using:

* Hybrid Retrieval (**BM25 + Cosine Similarity**)
* LLM reasoning with strict schema validation (**Pydantic v2**)
* Deterministic orchestration pipelines
* Artifact-first traceability
* Reusable Universal LLM Framework foundations

The system recommends ranked skills based on a user query, returning:

* Skill ID
* Skill Name
* Score
* LLM reasoning
* Evidence snippets
* Retrieval metadata

The architecture separates:

* Schema enforcement
* Batch indexing
* Online retrieval
* Context construction
* LLM generation
* Optional LLM judge
* Schema validation
* API serving
* Simple Web UI

This ensures modularity, testability, determinism, and production readiness.

---

# Pipeline Architecture

---

## 1) Schema Layer (Reusable Foundation)

These are inherited from the Universal LLM Framework.

---

### Pipeline 0 — Schema Ensure (Python)

Ensures `schema/llm_schema.py` exists and is valid.

Responsibilities:

* Generate or validate LLM output schema
* Enforce:

  * Pydantic v2
  * `ConfigDict(extra="forbid")`
  * Proper `__all__` exports
* Remove markdown fences
* Harden schema text deterministically

Purpose:

* Provides strict contract for structured LLM outputs.
* Used by prompt injection in later pipelines.

---

### Pipeline 1 — Schema Text Ensure

Ensures `schema/llm_schema.txt` exists.

Responsibilities:

* Convert `.py` schema to text form
* Clean formatting
* Prepare injection-ready schema text
* Guarantee stable formatting for prompt injection

Purpose:

* Injected into prompts to enforce JSON-only output.
* Keeps generation schema synchronized with Python schema.

---

## 2) Batch Layer

---

### Pipeline 2 — CSV Ingest + Vector Index Build

* Ingest skill taxonomy / corpus from CSV
* Generate embeddings
* Build FAISS index (local MVP backend)
* Build BM25 corpus artifacts
* Persist:

  * FAISS index
  * Metadata store
  * BM25 token store
* Save under `artifacts/`

Future:

* Replace FAISS → Vertex AI Vector Search
* Keep same retrieval interface

---

## 3) Online Layer (Runtime Pipelines)

These pipelines are invoked per API request.

---

### Pipeline 3a — Vector Search

Input:

* user query

Process:

* Embed query
* Perform cosine similarity search
* Return top_k vector candidates

Output:

* List of `RetrievedItem` (vector_score populated)

---

### Pipeline 3b — BM25 Search

Input:

* user query

Process:

* Tokenize query
* Perform BM25 search
* Return top_k lexical candidates

Output:

* List of `RetrievedItem` (bm25_score populated)

---

### Hybrid Merge (Deterministic)

Normalize both score types.

```
hybrid_score = alpha * normalized_vector_score
             + (1 - alpha) * normalized_bm25_score
```

Config-driven:

```
rag:
  hybrid:
    alpha: 0.6
```

Stable tie-break rules applied.

---

### Pipeline 4a — Context Builder + LLM Generation

Input:

* Merged retrieval results
* `generation.yaml`
* `llm_schema.txt`
* User query

Process:

* Construct strict system prompt
* Inject schema
* Inject retrieved context
* Call LLM via `functions/llm/runner.py`

Output:

* Raw structured JSON (string)

---

### Pipeline 4b — Optional Judge LLM

Optional evaluation stage.

Input:

* 4a input
* 4a output
* `judge.yaml`

Purpose:

* Quality validation
* Internal benchmarking
* Future scoring or refinement

---

### Pipeline 5 — Pydantic Validation + Final Output

* Parse JSON
* Validate against `recommendation_schema.py`
* Enforce:

  * strict fields
  * no extra keys
  * correct types
* Apply post-processing:

  * dedupe
  * clamp top_k
  * stable ordering

Return structured response.

---

# API Layer

---

## Endpoint

`POST /v1/recommend-skills`

---

### Request

* `query: str`
* `top_k: int`
* `language: Optional[str]`
* `filters: Optional[dict]`
* `debug: bool`

---

### Response

```
{
  "skills": [
    {
      "skill_id": "...",
      "skill_name": "...",
      "score": 0.87,
      "reason": "...",
      "evidence": ["...", "..."],
      "source": "taxonomy"
    }
  ],
  "meta": {
    "correlation_id": "...",
    "model": "...",
    "prompt_version": "...",
    "retrieval_version": "...",
    "timings": {...}
  }
}
```

---

# Repository Structure (Planned Additions)

Reusing:

* `functions/llm/`
* `functions/utils/`
* `functions/io/`
* `artifacts/`
* `configs/`
* `schema/llm_schema.*`

New additions:

```
batch/
  pipeline_2_ingest_and_index.py

online/
  pipeline_3a_vector_search.py
  pipeline_3b_bm25_search.py
  pipeline_4a_generation.py
  pipeline_4b_judge.py
  pipeline_5_validate.py

functions/
  rag/
    retriever.py
    bm25.py
    hybrid_ranker.py
    formatter.py
    types.py
  service/
    recommend_skills.py

schema/
  recommendation_schema.py

app/
  main.py
  middleware.py
  deps.py
  errors.py

ui/
  index.html
  app.js
  style.css
```

---

# Determinism & Traceability Guarantees

* Prompt versions tracked
* Retrieval version tracked
* Hybrid alpha in config
* Correlation ID middleware
* LLM outputs cached
* Failures stored
* Strict schema enforcement
* Stable ordering rules

---

# UI Plan (Simple Web GUI)

Purpose:

* Manual testing
* Demo capability
* Retrieval debugging

Features:

* Query input
* top_k control
* Debug toggle
* Ranked skill results
* Expandable evidence
* Optional retrieval debug panel

UI directly calls API via fetch.

No frontend framework required for MVP.

---

# Deployment Plan

Initial:

* Local FAISS backend

Later:

* Dockerize
* Deploy to Cloud Run
* Switch to Vertex AI Vector Search
* Secret Manager integration

Startup command:

```
uvicorn app.main:app --host 0.0.0.0 --port $PORT
```

---

# Design Principles Enforced

* Thin pipeline entrypoints
* Deterministic hybrid ranking
* Strict JSON-only LLM outputs
* Pydantic v2 with `extra="forbid"`
* Artifact-first debugging
* Config-driven behavior
* Retrieval backend abstraction

---

# Summary
The architecture is now defined.
Implementation begins with:
* Domain schema
* Batch indexing
* Retrieval layer
* API orchestration
* Minimal UI
This builds a clean, extensible Skill Recommendation system on top of the Universal LLM Framework foundation.