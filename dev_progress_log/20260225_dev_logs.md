# Skills Recommendation — Progress Summary (Full Handover)

> Status: **Pipelines 0–5 implemented, tested, and real-execution verified** ✅
> Status: **FastAPI online serving implemented + manual curl verified** ✅
> Date: **2026-02-25** (Asia/BKK)
> Repo: `skills_recommendation_via_rag`
> Output Guarantee: **Strict Pydantic v2 validation**, deterministic retrieval + ranking, **JSON-only** generation, optional judge gating, API-ready payload builder
> Test Status: **228 passed** ✅ (latest local run)
> Serving Status: **uvicorn + curl verified** ✅

---

## 0) What This Framework Is

This project implements a **Skill Recommendation API** using:

* Hybrid Retrieval (**Vector cosine via FAISS + BM25 lexical search**)
* LLM generation with strict schema validation (**Pydantic v2**, `extra="forbid"`)
* Optional LLM judge validation (JSON-only JudgeResult, strict)
* Deterministic, modular pipelines (batch + online)
* Artifact-first traceability (index store artifacts, cache artifacts, output artifacts)
* Reuse of Universal LLM Framework foundations (schema prompts + safe prompt renderer + JSON extraction + retries + cache)

The system recommends ranked skills based on a user query and returns:

* Skill ID + Skill Name
* Relevance score (0–1) + reasoning
* Evidence snippets copied from retrieved context
* Skill description (skill_text)
* Skill criteria (Foundational/Intermediate/Advanced)
* Optional debug metadata
* Optional judge gating (PASS required; judge details ignored in final payload)

Architecture separation (design principle: keep stages thin and testable):

* Schema enforcement (P0–P1)
* Batch ingestion + indexing (P2)
* Online retrieval (P3a–P3b)
* Hybrid merge + context construction (P4a)
* LLM generation + schema validation with retries + caching (P4b)
* LLM judge validation with retries + caching (P4c)
* API payload builder that enriches LLM output with retrieval meta (P5)
* FastAPI serving: expose only online pipelines (recommended: P5 endpoint)

---

## 0.1 Key Non-Functional Requirements

### Determinism

* Stable tie-break ordering across retrieval results
* Stable hybrid merge rules and normalization
* Stable context rendering (template-driven)
* Safe prompt rendering (brace-safe replacement; no `.format` corruption)
* Stable caching keys (hash-based deterministic cache_id per pipeline step)

### Safety & correctness

* Schema AST guard rejects unsafe imports/eval for schema generation (P0)
* Pydantic v2 strict validation prevents extra/unexpected keys
* LLM outputs must be JSON-only, schema conformant
* Judge output must be JSON-only with exact keys: verdict, score, reasons
* Online pipeline must not call schema regeneration (only uses existing schema files)
* Online retrieval layers do not call LLM (except P4b/P4c)

### Traceability

* Artifacts persist under `artifacts/`
* FAISS alignment invariant:

  * `faiss_meta.jsonl[i]` corresponds to FAISS internal id `i`
* BM25 corpus artifacts persist under `artifacts/index_store/`
* LLM outputs cached under `artifacts/cache/`
* Failure dumps under `artifacts/cache/_failures` (runner)
* API payload export under `artifacts/outputs/`

### Performance

* FAISS is exact search (IndexFlatIP) for MVP
* BM25 is in-process; built once per worker (process-local cache)
* Cloud Run cold start considerations:

  * BM25 index rebuild per worker cold start
  * FAISS store load per worker cold start
* Pipeline 4c optimized to avoid re-running 4b when possible
* Pipeline 5 optimized to avoid duplicate 4b by passing 4b payload into 4c

---

## 1) Repo Structure

Key directories:

* `functions/core/`

  * deterministic domain logic (no orchestration side-effects)
* `functions/online/`

  * thin wrappers intended for serving path
* `functions/batch/`

  * batch pipelines P0–P2
* `functions/utils/`

  * config, logging, hashing, path resolution, embeddings utilities
* `functions/llm/`

  * prompt loading + safe rendering
  * Gemini client factory
  * runner (prompt → call model → extract JSON/text → validate → cache/retry)
* `prompts/`

  * generation prompt YAML
  * judge prompt YAML
  * schema generation prompt YAML
* `schema/`

  * runtime schema `llm_schema.py`
  * injection schema `llm_schema.txt`
* `scripts/`

  * forced runs for pipelines 0–5
* `tests/`

  * unit tests for core and online pipelines

Git hygiene decisions:

* Goal: `git add .` must be safe
* `.gitignore` ignores:

  * `artifacts/**`
  * `archived/**`
  * embedding cache `.npy` batches
* `.gitkeep` used to keep folder structure without committing runtime outputs

---

## 2) Current Tree Snapshot (High-Level)

Observed current state includes:

* `functions/core/api_payload.py` ✅
* `functions/core/hybrid_merge.py` ✅
* `functions/core/context_render.py` ✅
* `functions/core/llm_generate.py` ✅
* `functions/core/llm_judge.py` ✅
* `functions/online/pipeline_4a_hybrid_context.py` ✅
* `functions/online/pipeline_4b_generate.py` ✅
* `functions/online/pipeline_4c_judge.py` ✅
* `functions/online/pipeline_5_api_payload.py` ✅
* `scripts/run_pipeline_5_force.py` saves payload to `artifacts/outputs/` ✅
* `app/main.py` FastAPI serving ✅

---

## 3) Pipeline Summary (0–5)

### Pipeline 0 — Schema Ensure (Python) ✅

Ensures `schema/llm_schema.py` exists and is valid.

Responsibilities:

* Generate or validate LLM output schema
* Enforce Pydantic v2 strictness:

  * `ConfigDict(extra="forbid")` injected into BaseModel subclasses
* Safe imports only:

  * AST guard rejects disallowed imports/eval
* Deterministic schema post-processing:

  * strip markdown fences
  * enforce exports
* Archive old schemas under `archived/`

Verified:

* Unit tests pass
* Real execution verified

Observations:

* Pipeline archives existing schema to timestamped file
* Schema byte size changes slightly per regeneration (expected)
* AFC enabled logs from google_genai

---

### Pipeline 1 — Schema Text Ensure ✅

Ensures `schema/llm_schema.txt` exists.

Responsibilities:

* Convert `.py` schema to injection-ready `.txt`
* Stable formatting for prompt injection
* Archive old schema txt under `archived/`

Verified:

* Unit tests pass
* Real execution verified

Observations:

* Archives old txt schema with timestamp
* Output used by generation prompt injection `{llm_schema}`

---

### Pipeline 2 — Ingest + Index Build (FAISS + BM25) ✅

Ingests `raw_data/skill_taxonomy.csv` and builds index store artifacts.

Persisted artifacts under `artifacts/index_store/`:

* `faiss.index`
* `faiss_meta.jsonl`
* `bm25_corpus.jsonl`
* `bm25_stats.json`
* `manifest.json`

Embedding config:

* Model: `gemini-embedding-001`
* Task type: `SEMANTIC_SIMILARITY`
* Dimension: 3072
* Verified docs: 38120

#### Pipeline 2 key responsibilities

* Read input CSV → DataFrame
* Validate required columns (config-driven)
* Build two distinct docs per row:

  * embedding doc (rich)
  * bm25 doc (compact)
* Embed all embedding docs (batch + cache)
* Build FAISS IndexFlatIP using normalized embeddings
* Persist aligned meta rows:

  * `faiss_meta.jsonl[i] ↔ FAISS internal id i`
* Persist BM25 corpus + stats
* Persist manifest

#### BM25 doc shaping decision (Option A)

Initial decision:

* BM25 should index compact doc, not rich embedding doc

Implementation:

* embedding doc contains criteria + scaffolding
* BM25 doc contains title + truncated skill_text

Observed effects in real runs:

* avgdl dropped ~267 → ~90
* candidates > 0 dropped ~23k → ~10k
* improved lexical cleanliness

#### BM25 criteria inclusion revision (Important change)

Later realization:

* If BM25 corpus excludes criteria fields entirely, 4b might lose access to criteria context
* Solution:

  * Keep BM25 searchable `text` compact (title + truncated skill_text)
  * But include additional non-search fields in BM25 row:

    * `Foundational_Criteria`
    * `Intermediate_Criteria`
    * `Advanced_Criteria`

Final BM25 row schema:

* `id`
* `title`
* `text` (compact searchable text)
* `source`
* `Foundational_Criteria`
* `Intermediate_Criteria`
* `Advanced_Criteria`

This allows:

* BM25 scoring remains clean
* Meta for context rendering + final API payload remains complete

---

### Pipeline 3a — Online Vector Search (FAISS) ✅

Goal:

* Given `query: str`, return vector hits from FAISS

Implementation:

* Core: `functions/core/vector_search.py`
* Store loader: `functions/core/index_store.py`
* Online wrapper: `functions/online/pipeline_3a_vector_search.py`

Core behavior:

* Validate query non-empty
* Validate top_k > 0
* Embed query online using repo embedding utils
* Enforce shape (1, dim)
* Optional normalize
* FAISS search yields indices + scores
* Filter invalid indices
* Map meta rows via internal_idx
* Hard-fail if meta missing (alignment invariant)
* Stable sort tie-break:

  * score desc
  * skill_id asc
  * internal_idx asc

Wrapper behavior:

* Supports config object or dict via `_get` `_get_path`
* Always resolves paths with:

  * `repo_root_from_parameters_path`
  * `resolve_path(..., base_dir=repo_root)`
* Loads store with process-local cache
* Embedding adapter is signature-aware (inspect):

  * does not assume task_type param exists in embedding adapter

Smoke verified:

* `scripts/run_pipeline_3a_force.py`
* Example query: “data scientist”
* Returns expected semantic skills

---

### Pipeline 3b — Online BM25 Search ✅

Goal:

* Lexical BM25 search over `bm25_corpus.jsonl`

Implementation:

* Core: `functions/core/bm25.py`
* Online wrapper: `functions/online/pipeline_3b_bm25_search.py`

BM25 corpus mapping in online wrapper:

* `id_key="id"`
* `name_key="title"`
* `doc_key="text"`
* `source_key="source"`

Core behavior:

* Tokenizer config:

  * lower
  * remove_punct
  * min_token_len
* Build index in-memory:

  * tf dict
  * df dict
  * idf with smoothing
* Score Okapi BM25
* Only keep score > 0 candidates
* Stable sort:

  * score desc
  * skill_id asc
  * internal_idx asc

Wrapper behavior:

* Process-local cache keyed by:

  * corpus path
  * bm25 params
  * mapping keys
* Path resolution CWD-independent
* Allows schema differences via fallback keys (core supports fallback)

Smoke verified:

* `scripts/run_pipeline_3b_force.py`
* Example query: “data scientist”
* Some lexical noise expected (handled by hybrid merge later)

---

### Pipeline 4a — Hybrid Merge + Context Builder ✅

Goal:

* Merge P3a vector hits + P3b BM25 hits into a hybrid list
* Build deterministic context string for LLM prompt injection

Implementation modules:

* `functions/core/hybrid_merge.py`
* `functions/core/context_render.py`
* Online wrapper: `functions/online/pipeline_4a_hybrid_context.py`

Hybrid merge behavior:

* Merge by `skill_id` (not internal_idx)
* Keep missing scores as 0.0
* Normalize scores deterministically:

  * min-max normalization per retrieval list
  * safe handling for empty/all-equal lists
* Compute hybrid score:

  * `score_hybrid = alpha * score_vector_norm + (1-alpha) * score_bm25_norm`
* Stable sort tie-break:

  * hybrid desc
  * vector desc
  * bm25 desc
  * id asc

Context rendering behavior:

* Uses `params.context.row_template`
* `columns.mode` include/exclude/all respected
* Stable field ordering:

  * deterministic field list
* Enforces:

  * `truncate_field_chars` per field
  * `max_context_chars` global
* Hard truncation fixed to ensure `len(context) <= max_context_chars`

  * Off-by-one bug found and fixed via unit test

Outputs:

* `results` merged list with scores + meta
* `context` string
* `debug` info:

  * raw min/max
  * counts
  * context length
  * truncated flag

Smoke verified:

* `scripts/run_pipeline_4a_force.py`

---

### Pipeline 4b — LLM Generation + Schema Validation ✅

Goal:

* Use `{context}` + `{llm_schema}` + `{query}` to generate skill recommendations
* Strict JSON-only output matching runtime schema
* Enforce Pydantic validation with retries
* Cache outputs deterministically

Implementation modules:

* Core: `functions/core/llm_generate.py`
* Online wrapper: `functions/online/pipeline_4b_generate.py`

Prompt:

* `prompts/generation.yaml`
* Injects:

  * `{llm_schema}`
  * `{query}`
  * `{context}`

Runner:

* `functions/llm/runner.py`:

  * loads YAML prompt via `functions/llm/prompts.py`
  * safe placeholder renderer (brace-safe)
  * calls Gemini
  * extracts JSON (raw_decode first object)
  * validates with Pydantic model
  * retries with corrective prefix
  * caches valid JSON

Schema model loading:

* `load_schema_model` in core loads schema module dynamically
* Important fix applied:

  * must call `model_rebuild()` to avoid Pydantic “not fully defined” errors when typing is used
  * also ensure correct model name:

    * runtime output model is `LLMOutput` (not `Output`)

Cache key:

* `build_cache_id_p4b` includes:

  * query
  * prompt_path
  * model_name
  * temperature
  * top_k
  * schema txt
  * context text

Config usage:

* `llm.model_name`
* `llm.temperature`
* `llm.max_retries`
* `cache.enabled`
* `cache.force`
* `cache.dump_failures`
* `cache.dir`

Real run observed:

* Returns validated JSON with keys:

  * analysis_summary
  * recommended_skills

---

### Pipeline 4c — LLM Judge Validation ✅

Goal:

* Evaluate the 4b output against:

  * schema compliance
  * grounding in context
  * evidence fidelity
  * sorting/score sanity
* Return JSON-only JudgeResult

Implementation modules:

* Core: `functions/core/llm_judge.py`
* Online wrapper: `functions/online/pipeline_4c_judge.py`

Prompt:

* `prompts/judge.yaml` updated for skill recommendation
* Injects:

  * `{llm_schema}`
  * `{query}`
  * `{context}`
  * `{output_json}` (stringified LLM output JSON)

Judge output schema:

* `JudgeResult` with keys:

  * verdict
  * score
  * reasons

Caching:

* deterministic `build_cache_id_p4c` includes:

  * query
  * prompt path
  * model name
  * temperature
  * context
  * output_json

Critical optimization added:

* Online wrapper now supports:

  * `p4b_payload: Optional[dict]`
* If provided:

  * 4c will **not** run 4b again
  * `used_provided_p4b_payload=True` in debug

Smoke verified:

* `scripts/run_pipeline_4c_force.py` revised to:

  * run 4b once
  * pass `p4b_payload` into 4c

---

### Pipeline 5 — API Payload Builder ✅

Goal:

* Produce final API JSON payload from pipeline output
* Each recommended skill must contain:

  * skill_id
  * skill_name
  * source
  * relevance_score
  * reasoning
  * evidence
  * skill_text
  * Foundational_Criteria
  * Intermediate_Criteria
  * Advanced_Criteria
* Ignore judge output once it passes (judge is gating only)

Implementation modules:

* Core: `functions/core/api_payload.py`
* Online wrapper: `functions/online/pipeline_5_api_payload.py`
* Script: `scripts/run_pipeline_5_force.py`

Join logic:

* Join LLM recs to retrieval meta by normalized skill_name
* Use best retrieval row if duplicates exist (prefer higher scores)
* `require_all_meta` flag:

  * if True, fail if missing meta
  * default False for robustness

Optimization applied:

* Pipeline 5 runs 4b once
* If `require_judge_pass=True`, pipeline 5 calls 4c and passes `p4b_payload=p4b`
* Result:

  * 4b is not executed twice
  * Serving cost reduced

Artifact export behavior:

* `scripts/run_pipeline_5_force.py` now:

  * clears `artifacts/outputs/`
  * writes payload JSON to `artifacts/outputs/pipeline5_api_payload__<ts>.json`

Real output verified includes:

* `skill_text` and all criteria fields
* join debug:

  * num_missing_meta = 0 in real runs

---

## 4) FastAPI Serving ✅

Goal:

* Expose only the **online pipelines** as API
* Recommended endpoint:

  * call pipeline 5
  * return pipeline 5 output (payload + meta + debug optional)

App file:

* `app/main.py`

Endpoints:

* `GET /healthz`
* `POST /v1/recommend-skills`

Request fields (current):

* query: string
* top_k: int
* debug: bool
* require_judge_pass: bool
* top_k_vector: int
* top_k_bm25: int
* require_all_meta: bool

Response fields:

* payload:

  * query
  * analysis_summary
  * recommended_skills array (enriched)
* meta:

  * generation_cache_id
* debug optional:

  * join stats
  * retrieval counts
  * etc.

Manual verification:

* `uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload`
* `curl -X POST http://localhost:8000/v1/recommend-skills ...`
* Response returned 200 OK
* Response content matches pipeline 5 output structure

---

## 5) Prompt System (Important Implementation Details)

### Safe placeholder rendering

* Implemented in `functions/llm/prompts.py`
* Uses brace-safe replacement:

  * does not break when injected blobs contain `{` or `}`
* Supports literal braces in templates via `{{` and `}}`
* Missing variables raise deterministic KeyError

### Prompt YAML loading

* `load_prompt_file(path)` returns dict
* Required `user` block
* Optional system block
* Sanitizes Unicode whitespace that can break YAML indentation:

  * NO-BREAK SPACE, BOM, etc.

### Runner retry mechanism

* On JSON parse/validation failure:

  * prepend corrective prefix to prompt
  * re-call Gemini
* Bounded by max_retries
* Dumps failures to `artifacts/cache/_failures` when enabled

---

## 6) Configuration State

### `configs/parameters.yaml`

Key sections:

* run:

  * timezone Asia/Bangkok
* input:

  * required columns include criteria columns
* context:

  * row_template includes:

    * skill_id
    * skill_name
    * source
    * score_vector
    * score_bm25
    * score_hybrid
    * skill_text
    * criteria blocks
  * max_context_chars (e.g. 30000)
  * truncate_field_chars (e.g. 2000)
* prompts:

  * generation.path = prompts/generation.yaml
  * judge.path = prompts/judge.yaml
* llm_schema:

  * py_path schema/llm_schema.py
  * txt_path schema/llm_schema.txt
  * force_regenerate true (dev mode)
* embeddings:

  * gemini-embedding-001
  * dim 3072
* rag:

  * vector_search.top_k_default
  * bm25 tokenizer params
  * hybrid alpha / normalization / tie_break
* index_store:

  * artifacts/index_store paths
* llm:

  * model gemini-3-flash-preview
  * temperature 1.0
  * retries 5
* cache:

  * dir artifacts/cache
  * enabled true

Important note on retrieval depth:

* `rag.vector_search.top_k_default` defines default retrieval depth for 3a/3b/4a/4b/5
* Output count from LLM is not forced to equal top_k; LLM may return fewer (e.g. 6–8)
* This is acceptable and expected
* Pipeline 5 returns however many skills LLM produced, enriched with meta

---

## 7) Testing & Verification

Test status:

* Latest: **228 passed**
* Coverage includes:

  * core ingestion doc builders
  * bm25 core
  * vector search core
  * hybrid merge core
  * context render core (includes truncation edge cases)
  * llm_generate core (mock runner)
  * llm_judge core (mock runner)
  * pipeline online wrappers (3a–5)
  * prompt renderer + runner extraction behavior

Real execution scripts:

* Pipeline 0:

  * scripts/run_pipeline_0_force.py
* Pipeline 1:

  * scripts/run_pipeline_1_force.py
* Pipeline 2:

  * scripts/run_pipeline_2_force.py
* Pipeline 3a:

  * scripts/run_pipeline_3a_force.py
* Pipeline 3b:

  * scripts/run_pipeline_3b_force.py
* Pipeline 4a:

  * scripts/run_pipeline_4a_force.py
* Pipeline 4b:

  * scripts/run_pipeline_4b_force.py
* Pipeline 4c:

  * scripts/run_pipeline_4c_force.py (reuses p4b_payload)
* Pipeline 5:

  * scripts/run_pipeline_5_force.py (saves payload JSON)

FastAPI manual test:

* uvicorn run
* curl request verified

---

## 8) Known Issues / Expected Behaviors

### BM25 lexical noise

* Lexical matches can still be weird
* Hybrid merge reduces impact via vector dominance (alpha=0.6)
* Judge can fail outputs that hallucinate unsupported skills if prompt drift occurs

### Judge score inflation

* With `temperature=1.0`, judge can be overly generous (often 100)
* In production:

  * set judge temperature lower (0.0–0.2)
  * consider separate config block for judge to decouple from generation temperature

### Schema regeneration noise

* P0/P1 can change schema slightly on each run
* Consider policy:

  * for production: set force_regenerate false
  * keep schema stable unless intentionally changed
  * commit schema to repo if needed for reproducibility

### Artifact size

* embedding cache `.npy` batches can be large
* Ensure `.gitignore` continues to exclude them

---

## 9) Dependency Management Notes

Issue encountered:

* `faiss-cpu==1.13.25` not found on pip
* Available versions include 1.13.2
* Fix:

  * pin to `faiss-cpu==1.13.2`
  * or range pin `faiss-cpu>=1.13.2,<1.14`

Potential issue:

* numpy 2.x compatibility with faiss wheels
* If import errors occur:

  * pin numpy `<2`
  * reinstall faiss-cpu

FastAPI deps added:

* fastapi
* uvicorn[standard]

---

## 10) Final Output Example (Pipeline 5)

Pipeline 5 output JSON file:

* `artifacts/outputs/pipeline5_api_payload__<timestamp>.json`

Contains:

* query
* analysis_summary
* recommended_skills list

  * skill_id
  * skill_name
  * source
  * relevance_score
  * reasoning
  * evidence
  * skill_text
  * Foundational_Criteria
  * Intermediate_Criteria
  * Advanced_Criteria

Example evidence property is strictly copied from context (prompt rule).

---

## 11) Serving Contract (FastAPI)

### Endpoint

* `POST /v1/recommend-skills`

### Request JSON

* query: string
* top_k: int (retrieval + context size)
* debug: bool
* require_judge_pass: bool
* top_k_vector: int
* top_k_bm25: int
* require_all_meta: bool

### Response JSON

* payload: object
* meta: object
* debug: optional object

---

## 12) Operational Runbooks

### Local dev run (end-to-end)

* Ensure env:

  * GEMINI_API_KEY available
* Run pipelines:

  * python scripts/run_pipeline_2_force.py (ensure index exists)
  * python scripts/run_pipeline_5_force.py (save payload)
* Run API:

  * uvicorn app.main:app --reload
* Curl test:

  * curl POST /v1/recommend-skills

### Reset artifacts/outputs

* pipeline 5 script clears `artifacts/outputs/` before writing

### Reset caches

* `scripts/clear_llm_cache.py` clears embedding caches and LLM caches
* `scripts/clear_archived.py` clears archived schema snapshots

---

## 13) Implementation Decisions (Historical)

### Config object vs dict access

* `load_parameters()` returns typed object in real execution
* tests sometimes monkeypatch dict
* solution:

  * `_get` + `_get_path` helpers support both

### Path resolution signature

* `resolve_path(path_like, *, base_dir)`
* Use:

  * `repo_root_from_parameters_path(parameters_path)`
  * `resolve_path(..., base_dir=repo_root)`
* Avoid assuming older signature

### Embedding adapter signature mismatch

* `GoogleEmbeddingModel.__init__` does not accept task_type
* fix:

  * inspect signature and only pass supported args

### BM25 corpus schema mapping

* artifact schema is:

  * id, title, text, source, criteria fields
* online BM25 must map:

  * id_key="id"
  * name_key="title"
  * doc_key="text"
  * source_key="source"

### Pydantic “not fully defined” error

* some schema models need rebuild
* fix:

  * call `model_rebuild()` after dynamic import
* also correct model_name:

  * use `LLMOutput` (not `Output`) if schema defines `LLMOutput`

### Redundant 4b calls in 4c/5

* initial state:

  * 4c ran 4b internally
  * 5 ran 4b and then 4c (thus 4b twice)
* fix:

  * 4c wrapper accepts `p4b_payload`
  * 5 passes `p4b_payload=p4b` into 4c

---

## 14) Current Status Summary

Completed and verified:

* ✅ P0: schema ensure
* ✅ P1: schema txt ensure
* ✅ P2: ingest + index build
* ✅ P3a: FAISS vector retrieval
* ✅ P3b: BM25 lexical retrieval
* ✅ P4a: hybrid merge + context render
* ✅ P4b: LLM generate + validate + cache + retry
* ✅ P4c: judge validate + cache + retry + optimized reuse
* ✅ P5: API payload builder (enrich LLM output with criteria + skill_text + ids)
* ✅ FastAPI:

  * /healthz
  * /v1/recommend-skills

Quality gates:

* ✅ 228 tests passed
* ✅ forced run scripts verified through P5
* ✅ uvicorn + curl verified

---

## 15) Future Enhancements (Optional)

* Add separate judge config block:

  * model_name_judge
  * temperature_judge
* Improve join robustness:

  * add `skill_id` to LLM schema to avoid name-join
* Add startup preloading for FastAPI:

  * warm FAISS + BM25 caches
* Add structured error codes for API:

  * 4xx for input errors
  * 5xx for internal errors
* Add OpenAPI schema export / docs link
* Add rate limiting / auth if needed for production

---
